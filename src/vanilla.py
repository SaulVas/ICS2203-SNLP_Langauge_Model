import xml.etree.ElementTree as ET
from collections import defaultdict
import json
import os
from frequency_counts import handle_sentence

class VanillaLM:
    """
    A language model that generates n-gram counts and calculates probabilities.

    This class represents a language model that generates n-gram counts based on sentences
    in a training set XML file. It calculates the probabilities of unigrams, bigrams, and
    trigrams based on the generated counts.

    Attributes:
        uni_probabilities (defaultdict): A dictionary to store unigram probabilities.
        bi_probabilities (defaultdict): A dictionary to store bigram probabilities.
        tri_probabilities (defaultdict): A dictionary to store trigram probabilities.

    Methods:
        __init__(): Initializes the language model and loads existing 
        n-gram counts or generates new ones.
        __str__(): Returns a string representation of the language model.
        generate_counts(): Generates n-gram counts and saves them to JSON files.
        uni_gram_prob(): Calculates unigram probabilities.
        bi_gram_prob(): Calculates bigram probabilities.
        tri_gram_prob(): Calculates trigram probabilities.
    """

    def __init__(self):
        """
        Initializes the language model and loads existing n-gram counts or generates new ones.

        If the n-gram count JSON files for unigrams, bigrams, and trigrams exist, they are loaded.
        Otherwise, new n-gram counts are generated by parsing the training_set.xml file.

        Args:
            self: The instance of the language model.

        Returns:
            None
        """
        self.uni_probabilities = defaultdict(int)
        self.bi_probabilities = defaultdict(int)
        self.tri_probabilities = defaultdict(int)

        if not (os.path.exists('n_grams/vanilla/1_gram_counts.json')
                and os.path.exists('n_grams/vanilla/2_gram_counts.json')
                and os.path.exists('n_grams/vanilla/3_gram_counts.json')):
            self.generate_counts()
        else:
            with open("n_grams/vanilla/1_gram_counts.json", 'r', encoding='utf-8') as fp:
                self.uni_count = json.load(fp)
            with open("n_grams/vanilla/2_gram_counts.json", 'r', encoding='utf-8') as fp:
                self.bi_count = json.load(fp)
            with open("n_grams/vanilla/3_gram_counts.json", 'r', encoding='utf-8') as fp:
                self.tri_count = json.load(fp)

        self.uni_gram_prob()
        self.bi_gram_prob()
        self.tri_gram_prob()

    def __str__(self) -> str:
        ret_str =  (f"uni_count has {len(self.uni_count.keys())} tokens\n"
                + f"bi_count has {len(self.bi_count.keys())} tokens\n"
                + f"tri_count has {len(self.tri_count.keys())} tokens\n")
        return ret_str

    def generate_counts(self):
        """
        Generate n-gram counts and save them to JSON files.

        This function iterates over a range of word counts (1 to 3) and generates n-gram counts
        based on the sentences in the training_set.xml file. The n-gram counts are then saved
        to separate JSON files for each word count.

        Args:
            self: The instance of the language model.

        Returns:
            None
        """
        for number_of_words in range(1, 4):
            n_gram_counts = defaultdict(int)
            tree = ET.parse('../data/training_set.xml')
            root = tree.getroot()
            for child in root:
                handle_sentence(child, number_of_words, n_gram_counts)

            with open(f'n_grams/vanilla/{number_of_words}_gram_counts.json',
                    'w', encoding='utf-8') as fp:
                json.dump(n_gram_counts, fp, indent=4)

    def uni_gram_prob(self):
        """
        Calculates unigram probabilities.

        The function calculates the probability of each unigram based on the total token count.

        Args:
            self: The instance of the language model.

        Returns:
            None
        """
        total_tokens = float(sum(self.uni_count.values()))
        for key in self.uni_count:
            self.uni_probabilities[key] = self.uni_count[key] / total_tokens

    def bi_gram_prob(self):
        """
        Calculates bigram probabilities.

        The function calculates the probability of each bigram based on the bigram counts and
        corresponding unigram counts.

        Args:
            self: The instance of the language model.

        Returns:
            None
        """
        for key in self.bi_count:
            words = tuple(key.split())
            self.bi_probabilities = self.bi_count[key] / self.uni_count[words[-1]]

    def tri_gram_prob(self):
        """
        Calculates trigram probabilities.

        The function calculates the probability of each trigram based on the trigram counts and
        corresponding bigram counts.

        Args:
            self: The instance of the language model.

        Returns:
            None
        """
        for key in self.tri_count:
            words = tuple(key.split())
            bi_gram_key = words[0] + " " + words[1]
            self.tri_probabilities = self.tri_count[key] / self.bi_count[bi_gram_key]

Vanilla = VanillaLM()
print(Vanilla)
