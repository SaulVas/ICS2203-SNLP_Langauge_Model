{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models Documentation\n",
    "\n",
    "## Project Structure\n",
    "The project is split into 3 directories:\n",
    "\n",
    "- data - contains the british corpus and the training and test data sets.\n",
    "- documentation - contains any files used for testing purposes or related to the documentation\n",
    "- src - contains the source code of the project.\n",
    "\n",
    "Within src there is a file for the abstract base class of a language model, a file for each language model class, the main file *main.py*, and a file containing all the functions created used to manipulate the data sets, *dataset_functions.py*. There is also a folder containing all the n_gram counts for each model and the corpus counts in JSON format.\n",
    "\n",
    "## Part 1 - Generating Counts\n",
    "\n",
    "Initially for the first part of this project I will be generating counts for the corpus as a whole and will not be splitting the data set, however, this process is designed in a way that it can extract counts from any xml file of similar structure. Thus, later in the project when I am training the models on a training set I was able to reuse the same functions. These corpus counts can be found under *src/n_grams/corpus*.\n",
    "\n",
    "In my implementation I have opted to store the generated counts in JSON files so that this expensive computational section can be skipped in testing. I do this by checking if the JSON files exist, if they do I simply load the counts from the json into a dictionary and if they dont I create them. Due to this choice I am storing the counts in dictionaries, with the n_gram in a string as a key and the count as the value. Later on when I calculate and generate probabilities I will key the dictionary with tuples containing each token of the ngram rather than 1 string.\n",
    "\n",
    "Generating the corpus counts is done using the following 4 functions:\n",
    "\n",
    "- generate_corpus_counts()\n",
    "- traverse_tree(node, number_of_words, counts)\n",
    "- handle_sentence(sentence_node, number_of_words, counts)\n",
    "- retrieve_text(node)\n",
    "\n",
    "### generate_corpus_counts()\n",
    "This function starts by parsing either a single or multiple *.xml* files which contain the training data/corpus for this project. Once the root of each *.xml* file has been found, the function iterates over a loop three times, generating a frequency count for each n_gram *(unigram, bigram, trigram)*. \n",
    "This is achieved by calling the traverse_tree() function for each of the roots children. \n",
    "\n",
    "Once the frequency count dictionary has been created, it is stored in a json file so that in the future this process doesn't need to be repeated everytime the language model is to be used.\n",
    "\n",
    "### traverse_tree(node, number_of_words, counts)\n",
    "This function takes 3 parameters:\n",
    "\n",
    "- node (xml.etree.ElementTree.Element): The current node in the XML tree.\n",
    "- number_of_words (int): The number of words in the n-grams to be counted.\n",
    "- counts (collections.defaultdict): A dictionary to store the n-gram counts.\n",
    "\n",
    "This function is a recursive depth first function that traverses the tree until it finds either a *teiHeader* tag or an *s* tag. Due to the structure of the *.xml* files in the corpus, the first child of the root (*teiHeader*) contains information about the text and its origins. This is irrelevant to our frequency counts so we simply ignore it and keep searching the tree until an *s* tag is found. Once found the handle_sentence() function is called.\n",
    "\n",
    "### handle_sentence(sentence_node, number_of_words, counts)\n",
    "This function takes 3 parameters:\n",
    "\n",
    "- sentence_node (xml.etree.ElementTree.Element): The current sentence node in the XML tree.\n",
    "- number_of_words (int): The size of the ngram counts to be extracted.\n",
    "- counts (collections.defaultdict): A dictionary in which to store the n-gram counts.\n",
    "  \n",
    "The function calls retrieve_text() and recieves the sentence in string format, processes each sentence by stripping it and adding opening and closing sentence tokens, and then computes and updates n-gram frequencies stored in the counts variable. It will only update n-gram frequencies if the sentence contained words, if no words were present then it moves onto the next sentence.\n",
    "\n",
    "### retrieve_text(node)\n",
    "This function takes one parameter:\n",
    "\n",
    "- node (xml.etree.ElementTree.Element): The current node in the XML tree.\n",
    "\n",
    "It returns a str, which represents the concatenated text from the node and its descendants. It does this by recursively checking all of the nodes children for word tags and ignores punctuation in the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "directories = ['aca', 'dem', 'fic', 'news']\n",
    "BASE_PATH = '../data/corpus/Texts/'\n",
    "\n",
    "def generate_corpus_counts():\n",
    "    for number_of_words in range(1, 4):\n",
    "        n_gram_counts = defaultdict(int)\n",
    "\n",
    "        for directory in directories:\n",
    "            dir_path = os.path.join(BASE_PATH, directory)\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith('.xml'):\n",
    "                    file_path = os.path.join(dir_path, file)\n",
    "                    tree = ET.parse(file_path)\n",
    "                    root = tree.getroot()\n",
    "                    for child in root:\n",
    "                        if child.tag != 'teiHeader':\n",
    "                            traverse_tree(child, number_of_words, n_gram_counts)\n",
    "\n",
    "        with open(f'n_grams/corpus/{number_of_words}_gram_counts.json',\n",
    "                  'w', encoding='utf-8') as fp:\n",
    "            json.dump(n_gram_counts, fp, indent=4)\n",
    "\n",
    "def traverse_tree(node, number_of_words, counts):\n",
    "    \"\"\" Recursively traverses the XML tree to find sentences and process their \n",
    "    text for n-gram frequency calculation.\n",
    "\n",
    "    Parameters:\n",
    "    node (xml.etree.ElementTree.Element): The current node in the XML tree.\n",
    "    number_of_words (int): The number of words in the n-grams to be counted.\n",
    "    counts (collections.defaultdict): A dictionary to store the n-gram counts.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for child in node:\n",
    "        if child.tag == 's':\n",
    "            handle_sentence(child, number_of_words, counts)\n",
    "        else:\n",
    "            traverse_tree(child, number_of_words, counts)\n",
    "\n",
    "def handle_sentence(sentence_node, number_of_words, counts):\n",
    "    \"\"\" Processes each sentence to compute and update n-gram frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    sentence_node (xml.etree.ElementTree.Element): The current sentence node in the XML tree.\n",
    "    number_of_words (int): The number of words in the n-grams to be counted.\n",
    "    counts (collections.defaultdict): A dictionary to store the n-gram counts.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    text = retrieve_text(sentence_node)\n",
    "    if text.strip() != \"\":\n",
    "        text = (\"<s> \" * number_of_words) + text + (\" </s>\")\n",
    "        words = text.split()\n",
    "        for index in range(len(words) - number_of_words + 1):\n",
    "            if number_of_words == 1:\n",
    "                n_gram = words[index]\n",
    "            else:\n",
    "                n_gram = \" \".join(words[index:index + number_of_words])\n",
    "            counts[n_gram] += 1\n",
    "\n",
    "def retrieve_text(node):\n",
    "    \"\"\" Extracts and concatenates text from XML nodes, adding start and end \n",
    "    markers to each sentence.\n",
    "\n",
    "    Parameters:\n",
    "    node (xml.etree.ElementTree.Element): The current node in the XML tree.\n",
    "\n",
    "    Returns:\n",
    "    str: The concatenated text from the node and its descendants.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    for child in node:\n",
    "        if child.tag == 'w':\n",
    "            if child.text:\n",
    "                text += child.text.lower()\n",
    "        elif child.tag == 'c':\n",
    "            text += \" \"\n",
    "        else:\n",
    "            if len(node) > 0:\n",
    "                for grandchild in child:\n",
    "                    text += retrieve_text(grandchild)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stages of Implementation\n",
    "\n",
    "Obviously it was not a good idea to start with trying to process the whole corpus all at once. Thus, I initially started by extracting 1 sentence node from a custom made xml file *documentation/test_1.xml*. Then I moved onto extracting the sentences from 1 file from the corpus *test_2.xml*. Finally, when both of these tests were passed I moved onto processing the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Before extending the initial *single file* implementation, the json files were checked to verify that the generated counts were correct.\n",
    "\n",
    "Finally once the implemenation was finished, the performance of this section was tested using 2 different metrics.\n",
    "1. Time taken to create each n_gram\n",
    "2. CPU and RAM usage\n",
    "\n",
    "The extremely long time taken to generate the counts of the whole corpus is the reason behind the decision to generate these counts once and then store them for future use. Another reason for this is that the counts will not change, once the data set has been split the counts for each model will be consistent. The probabilities will also be consistent, however, for performance reasons we want the probabilities to be keyed using tuples since accessing the tokens is much easier this way.\n",
    "\n",
    "The results for time and computer resources usage are below.\n",
    "\n",
    "### Time Taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timed_single_file_frequency_counts(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Create frequency counts for unigrams, bigrams, and trigrams and save to a json file\n",
    "    for number_of_words in range(1, 4):\n",
    "        start_time = time.time()\n",
    "\n",
    "        n_gram_counts = defaultdict(int)\n",
    "        traverse_tree(root, number_of_words, n_gram_counts)\n",
    "\n",
    "        end_time = time.time()  \n",
    "        print(f\"Time to process {number_of_words}_gram: {end_time - start_time} seconds\")\n",
    "\n",
    "def timed_corpus_frequency_counts():\n",
    "    directories = ['aca', 'dem', 'fic', 'news']\n",
    "    BASE_PATH = '../data/corpus/Texts/' \n",
    "\n",
    "    # Create frequency counts for unigrams, bigrams, and trigrams\n",
    "    for number_of_words in range(1, 4):\n",
    "        start_time = time.time()\n",
    "        n_gram_counts = defaultdict(int)\n",
    "\n",
    "        for directory in directories:\n",
    "            dir_path = os.path.join(BASE_PATH, directory)\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith('.xml'):\n",
    "                    file_path = os.path.join(dir_path, file)\n",
    "                    tree = ET.parse(file_path)\n",
    "                    root = tree.getroot()\n",
    "                    for child in root:\n",
    "                        if child.tag != 'teiHeader':\n",
    "                            traverse_tree(child, number_of_words, n_gram_counts)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Time to process {number_of_words}_gram: {end_time - start_time} seconds\")\n",
    "\n",
    "print(\"Test 1:\")\n",
    "timed_single_file_frequency_counts(\"test_1.xml\")\n",
    "print()\n",
    "\n",
    "print(\"Test 2:\")\n",
    "timed_single_file_frequency_counts(\"test_2.xml\")\n",
    "print()\n",
    "\n",
    "print(\"Test 3:\")\n",
    "timed_corpus_frequency_counts()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def single_file_frequency_counts(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Create frequency counts for unigrams, bigrams, and trigrams\n",
    "    for number_of_words in range(1, 4):\n",
    "        n_gram_counts = defaultdict(int)\n",
    "        for child in root:\n",
    "            traverse_tree(child, number_of_words, n_gram_counts)\n",
    "            \n",
    "def corpus_frequency_counts():\n",
    "    test_dir_path = \"test_3/\"\n",
    "    xml_files = [f for f in os.listdir(test_dir_path) if f.endswith('.xml')]\n",
    "\n",
    "        # Create frequency counts for unigrams, bigrams, and trigrams\n",
    "    for number_of_words in range(1, 4):\n",
    "        n_gram_counts = defaultdict(int)\n",
    "\n",
    "        for xml_file in xml_files:\n",
    "            path = os.path.join(test_dir_path, xml_file)\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                traverse_tree(child, number_of_words, n_gram_counts)\n",
    "\n",
    "def test_cpu_usage_single_file(path):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_cpu = process.cpu_percent()\n",
    "\n",
    "    single_file_frequency_counts(path)\n",
    "\n",
    "    end_cpu = process.cpu_percent()\n",
    "\n",
    "    cpu_usage = end_cpu - start_cpu\n",
    "\n",
    "    print(f\"CPU Usage: {cpu_usage}%\")\n",
    "\n",
    "def test_ram_usage_single_file(path):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    single_file_frequency_counts(path)\n",
    "\n",
    "    end_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    ram_usage = end_ram - start_ram\n",
    "\n",
    "    print(f\"RAM Usage: {ram_usage} MB\")\n",
    "\n",
    "def test_cpu_usage_corpus():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_cpu = process.cpu_percent()\n",
    "\n",
    "    corpus_frequency_counts()\n",
    "\n",
    "    end_cpu = process.cpu_percent()\n",
    "\n",
    "    cpu_usage = end_cpu - start_cpu\n",
    "\n",
    "    print(f\"CPU Usage: {cpu_usage}%\")\n",
    "\n",
    "def test_ram_usage_corpus():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    corpus_frequency_counts()\n",
    "\n",
    "    end_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    ram_usage = end_ram - start_ram\n",
    "\n",
    "    print(f\"RAM Usage: {ram_usage} MB\")\n",
    "\n",
    "print(\"One lines test:\")\n",
    "test_cpu_usage_single_file(\"test_1.xml\")\n",
    "test_ram_usage_single_file(\"test_1.xml\")\n",
    "\n",
    "print(\"Single file test:\")\n",
    "test_cpu_usage_single_file(\"test_2.xml\")\n",
    "test_ram_usage_single_file(\"test_2.xml\")\n",
    "\n",
    "print(\"Corpus Test:\")\n",
    "test_cpu_usage_corpus()\n",
    "test_ram_usage_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Datasets\n",
    "\n",
    "I am using the british corpus which consists of a total of 332963 sentence tags. For my project, since in my linear interpolation function the lambdas are preset and dont need to be trained, I am opting to simply split my data set into a training and test set. The split I opted for is 80|20.\n",
    "\n",
    "Ive also opted to split my data set randomly. Since for this project we mostly dont care about continuity and context of a phrase, I chose to randomly select 80% of the sentences from each file.\n",
    "\n",
    "The splitting of the data set is handled using 2 functions:\n",
    "- splitting_datasets()\n",
    "- split_and_append_elements(s_elements, training_set, test_set):\n",
    "\n",
    "### splitting_datasets()\n",
    "\n",
    "This function simply loops through all the files of the corpus, extracts all the sentences from it, and calls split_and_append_elements().\n",
    "\n",
    "### split_and_append_elements(s_elements, training_set, test_set)\n",
    "\n",
    "It takes 3 parameters:\n",
    "\n",
    "- s_elements - list of sentence elements\n",
    "- trainin_set - array representing the training set\n",
    "- testing_set - array representing the testing set\n",
    "\n",
    "This function shuffles the initial sentences randomly, and then splits them based on the desired percentages, appending the split lists to the respective global lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def splitting_datasets():\n",
    "    \"\"\"\n",
    "    Splits the corpus into train and test sets and saves them as XML files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    train_file_path = '../data/training_set.xml'\n",
    "    test_file_path = '../data/test_set.xml'\n",
    "    # Splitting the corpus into train, validation, and test sets if not already created\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    for directory in directories:\n",
    "        dir_path = os.path.join(BASE_PATH, directory)\n",
    "        for file in os.listdir(dir_path):\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(dir_path, file)\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                sentences = list(root.findall('.//s'))\n",
    "                split_and_append_elements(sentences, train, test)\n",
    "\n",
    "    if os.path.exists(train_file_path):\n",
    "        os.remove(train_file_path)\n",
    "    if os.path.exists(test_file_path):\n",
    "        os.remove(test_file_path)\n",
    "    write_xml_from_elements(train, train_file_path)\n",
    "    write_xml_from_elements(test, test_file_path)\n",
    "\n",
    "def split_and_append_elements(s_elements, training_set, test_set):\n",
    "    \"\"\"\n",
    "    Splits the given list of elements into train, test, and test sets,\n",
    "    and appends them to the respective global lists.\n",
    "\n",
    "    Args:\n",
    "        s_elements (list): The list of elements to be split.\n",
    "        training_set (list): The list to append the training set elements to.\n",
    "        test_set (list): The list to append the test set elements to.\n",
    "        test_set (list): The list to append the test set elements to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    total_elements = len(s_elements)\n",
    "    train_size = int(total_elements * 0.8)\n",
    "\n",
    "    random.shuffle(s_elements)\n",
    "    train_elements = s_elements[:train_size]\n",
    "    test_elements = s_elements[train_size:]\n",
    "\n",
    "    training_set.extend(train_elements)\n",
    "    test_set.extend(test_elements)\n",
    "\n",
    "def write_xml_from_elements(elements, path):\n",
    "    root = ET.Element('root')\n",
    "    root.extend(elements)\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "The 3 language models of my project are all built upon a language model abstract base class. This ABC implements the backbone of all of the models. The most important functions implemented are:\n",
    "\n",
    "- \\__init__(self)\n",
    "- _get_counts(self)\n",
    "- _generate_counts(self)\n",
    "- _linear_interpolation(self, trigram)\n",
    "- text_generator(self, sentence, choice)\n",
    "- _get_probable_tokens(self, context, choice)\n",
    "- uni_sentence_probability(self, words)\n",
    "- bi_sentence_probability(self, words)\n",
    "- tri_sentence_probability(self, words)\n",
    "- sentence_probability(self, words)\n",
    "\n",
    "The language models are then left to simply define how they calculate the probabilities from the counts by implementing the following abstract methods:\n",
    "\n",
    "- _default_uni_value(self)\n",
    "- _get_bigram_probability(self, bigram)\n",
    "- _get_trigram_probability(self, trigram)\n",
    "- _generate_unigram_probs(self)\n",
    "- _generate_bigram_probs(self)\n",
    "- _generate_trigram_probs(self)\n",
    "\n",
    "### \\_init_(self)\n",
    "\n",
    "The init method simply defines 6 default dictionaries, one for each ngram count, and one for each ngram probabilities. Then it proceeds to populate these dictionaries by calling _get_counts(), _generate_unigram_probs(), _generate_bigram_probs() and _generate_trigram_probs().\n",
    "\n",
    "### \\_get_counts(self)\n",
    "\n",
    "This is method checks if the ngram counts have already been created for the vanilla/laplace model and loads them if so, otherwise it calls self.\\_generate_counts()\n",
    "\n",
    "### \\_generate_counts(self)\n",
    "\n",
    "This method iterates over a range of word counts (1 to 3) and generates n-gram counts based on the sentences in the training_set.xml file. The n-gram counts are then saved to separate JSON files for each word count. It does this using the handle_sentence() method discussed in part 1.\n",
    "\n",
    "### \\_linear_interpolation(self, trigram)\n",
    "\n",
    "This method returns the probability of a trigram using linear interpolation. It does this by calling the abstract methods implemented by the models themselves.\n",
    "\n",
    "### text_generator(self, sentence, choice)\n",
    "\n",
    "This section was inspired by the first link in the references section, however, it is still my own work [1].\n",
    "This method handles the text generation of the model, it accepts a starting phrase and a choice that determines whether the user wants to use unigram, bigram, trigram or linear interpolation to determine the next word. \n",
    "The method first makes sure that the sentence supplied is converted to a list of words and adds a start tag to the beginning of it. Then it jumps to the last 2 tokens and generates a tuple for the context.\n",
    "Next it gets the most probable tokens by calling self._get_probable_tokens(). Then it selects the next token based on its normalised probability of being chosen. The context tuple is updated and the loop continues until either no word is generated, and end tag is generated or over 100 words are generated.\n",
    "\n",
    "### \\_get_probable_tokens(self, context, choice)\n",
    "\n",
    "This method creates a dictionary of all the possible next tokens and their probabilities of being selected. It does this based on the choice selected by the user. In the case that linear interpolation is selected, if the a trigram exists that starts with the context supplied as an argument, than the probability of the token is calculated using the linear interpolation method.\n",
    "\n",
    "### uni_sentence_probability(self, words), bi_sentence_probability(self, words) and tri_sentence_probability(self, words)\n",
    "\n",
    "These methods return the probability of a sentence using the following formula:\n",
    "\n",
    "$$P(S) = P(W1)P(W1|CONTEXT)P(W2|CONTEXT)...P(Wn|CONTEXT)$$\n",
    "\n",
    "Where context is determined by the size of the ngram being used.\n",
    "\n",
    "### sentence_probability(self, words)\n",
    "\n",
    "This method returns the probability of a sentence using the following formula:\n",
    "\n",
    "$$P(S) = P(W1)P(W2)P(W3)...P(Wn-1)P(Wn)$$\n",
    "\n",
    "Where the probability of a word is found using linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements an abstract base class for language models\n",
    "\"\"\"\n",
    "import string\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "class LanguageModel(ABC):\n",
    "    def __init__(self):\n",
    "        self.uni_count = defaultdict(int)\n",
    "        self.bi_count = defaultdict(int)\n",
    "        self.tri_count = defaultdict(int)\n",
    "        self.uni_probabilities = defaultdict(self._default_uni_value)\n",
    "        self.bi_probabilities = defaultdict(float)\n",
    "        self.tri_probabilities = defaultdict(float)\n",
    "\n",
    "        self._get_counts()\n",
    "        self._generate_unigram_probs()\n",
    "        self._generate_bigram_probs()\n",
    "        self._generate_trigram_probs()\n",
    "\n",
    "    def __str__(self):\n",
    "        ret_str =  (f\"uni_count has {len(self.uni_count.keys())} tokens\\n\"\n",
    "                + f\"bi_count has {len(self.bi_count.keys())} tokens\\n\"\n",
    "                + f\"tri_count has {len(self.tri_count.keys())} tokens\\n\")\n",
    "        return ret_str\n",
    "\n",
    "    @abstractmethod\n",
    "    def _default_uni_value(self):\n",
    "        pass\n",
    "\n",
    "    def _get_counts(self):\n",
    "        if not (os.path.exists('n_grams/vanilla_laplace/1_gram_counts.json')\n",
    "                and os.path.exists('n_grams/vanilla_laplace/2_gram_counts.json')\n",
    "                and os.path.exists('n_grams/vanilla_laplace/3_gram_counts.json')):\n",
    "            self._generate_counts()\n",
    "\n",
    "        with open(\"n_grams/vanilla_laplace/1_gram_counts.json\", 'r', encoding='utf-8') as fp:\n",
    "            self.uni_count = json.load(fp)\n",
    "        with open(\"n_grams/vanilla_laplace/2_gram_counts.json\", 'r', encoding='utf-8') as fp:\n",
    "            self.bi_count = json.load(fp)\n",
    "        with open(\"n_grams/vanilla_laplace/3_gram_counts.json\", 'r', encoding='utf-8') as fp:\n",
    "            self.tri_count = json.load(fp)\n",
    "\n",
    "    def _generate_counts(self):\n",
    "        for number_of_words in range(1, 4):\n",
    "            n_gram_counts = defaultdict(int)\n",
    "            tree = ET.parse('../data/training_set.xml')\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                handle_sentence(child, number_of_words, n_gram_counts)\n",
    "\n",
    "            with open(f'n_grams/vanilla_laplace/{number_of_words}_gram_counts.json',\n",
    "                    'w', encoding='utf-8') as fp:\n",
    "                json.dump(n_gram_counts, fp, indent=4)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _generate_unigram_probs(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _generate_bigram_probs(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _generate_trigram_probs(self):\n",
    "        pass\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        punctuation = string.punctuation.replace(\"'\", \"\")\n",
    "        text_without_punctuation = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "\n",
    "        return text_without_punctuation\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_bigram_probability(self, bigram):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_trigram_probability(self, trigram):\n",
    "        pass\n",
    "\n",
    "    def _linear_interpolation(self, trigram):\n",
    "        uni_prob = 0.1 * self.uni_probabilities[trigram[-1]]\n",
    "        bi_prob = 0.3 * self._get_bigram_probability(trigram[-2:])\n",
    "        tri_prob = 0.6 * self._get_trigram_probability(trigram)\n",
    "        return uni_prob + bi_prob + tri_prob\n",
    "\n",
    "    def text_generator(self, sentence, choice):\n",
    "        words = sentence\n",
    "        if not isinstance(words, list):\n",
    "            words = self._remove_punctuation(words)\n",
    "            words = words.lower().split()\n",
    "\n",
    "        words.insert(0, \"<s>\")\n",
    "        if len(words) > 1:\n",
    "            context = tuple(words[-2:])\n",
    "            loop_prevention_counter = 0\n",
    "\n",
    "            while context[-1] not in [\"</s>\", \"\"] and loop_prevention_counter < 100:\n",
    "                token_probabilities = self._get_probable_tokens(context, choice)\n",
    "\n",
    "                if not token_probabilities:\n",
    "                    break\n",
    "\n",
    "                if token_probabilities[\"<s>\"] != 0:\n",
    "                    del token_probabilities[\"<s>\"]\n",
    "\n",
    "                # semi-random selection of next word based on normalised probability\n",
    "                prob_sum = sum(token_probabilities.values())\n",
    "                random_dec = random.random()\n",
    "                running_sum = 0\n",
    "                for token, probability in sorted(token_probabilities.items(),\n",
    "                                                 key=lambda item: item[1]):\n",
    "                    running_sum += np.divide(probability, prob_sum)\n",
    "                    if running_sum > random_dec:\n",
    "                        word = token\n",
    "                        break\n",
    "\n",
    "                words.append(word)\n",
    "                context = (context[-1], word)\n",
    "                loop_prevention_counter += 1\n",
    "\n",
    "        if words[-1] != \"</s>\":\n",
    "            words.append(\"</s>\")\n",
    "\n",
    "        print(\" \".join(words))\n",
    "\n",
    "    def _get_probable_tokens(self, context, choice):\n",
    "        token_probabilities = defaultdict(float)\n",
    "\n",
    "        if choice == '1':\n",
    "            for key, value in self.uni_probabilities.items():\n",
    "                token_probabilities[key] = value\n",
    "        elif choice == '2':\n",
    "            for key, value in self.bi_probabilities.items():\n",
    "                if key[0] == context[-1]:\n",
    "                    token_probabilities[key[-1]] = value\n",
    "        elif choice == '3':\n",
    "            for key, value in self.tri_probabilities.items():\n",
    "                if key[:2] == context:\n",
    "                    token_probabilities[key[-1]] = value\n",
    "        elif choice == '4':\n",
    "            for key in self.tri_probabilities:\n",
    "                if key[0:2] == context:\n",
    "                    token_probabilities[key[-1]] = self._linear_interpolation(key)\n",
    "\n",
    "        return token_probabilities\n",
    "\n",
    "    def uni_sentence_probability(self, words):\n",
    "        if not isinstance(words, list):\n",
    "            words = self._remove_punctuation(words.lower())\n",
    "            words = words.split()\n",
    "\n",
    "        sentence_probability = 1\n",
    "        for unigram in words:\n",
    "            prob = self.uni_probabilities[unigram]\n",
    "            if prob == 0:\n",
    "                prob = self.uni_probabilities[\"<UNK>\"]\n",
    "            sentence_probability *= prob\n",
    "\n",
    "        return sentence_probability\n",
    "\n",
    "    def bi_sentence_probability(self, words):\n",
    "        if not isinstance(words, list):\n",
    "            words = self._remove_punctuation(words.lower())\n",
    "            words = [\"<s>\"] + words.split() + [\"</s>\"]\n",
    "\n",
    "        sentence_probability = 1\n",
    "        for index in range(len(words) - 2):\n",
    "            bigram = tuple(words[index : index+2])\n",
    "            prob = self._get_bigram_probability(bigram)\n",
    "            sentence_probability *= prob\n",
    "\n",
    "        return sentence_probability\n",
    "\n",
    "    def tri_sentence_probability(self, words):\n",
    "        if not isinstance(words, list):\n",
    "            words = self._remove_punctuation(words.lower())\n",
    "            words = [\"<s>\", \"<s>\"] + words.split() + [\"</s>\"]\n",
    "\n",
    "        sentence_probability = 1\n",
    "        for index in range(len(words) - 3):\n",
    "            trigram = tuple(words[index : index+3])\n",
    "            prob = self._get_trigram_probability(trigram)\n",
    "            sentence_probability *= prob\n",
    "\n",
    "        return sentence_probability\n",
    "\n",
    "    def sentence_probability(self, words):\n",
    "        if not isinstance(words, list):\n",
    "            words = self._remove_punctuation(words.lower())\n",
    "            words = [\"<s>\", \"<s>\"] + words.split() + [\"</s>\"]\n",
    "\n",
    "        sentence_probability = 1\n",
    "        for index in range(len(words) - 3):\n",
    "            trigram = tuple(words[index : index+3])\n",
    "            prob = self._linear_interpolation(trigram)\n",
    "            sentence_probability *= prob\n",
    "\n",
    "        return sentence_probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla LM\n",
    "\n",
    "This class extends the LM ABC and implements the abstract methods in their simplest forms.\n",
    "\n",
    "### \\_default_uni_value(self)\n",
    "\n",
    "This method simply returns 0.0, the default value for the default dictionary.\n",
    "\n",
    "### _generate_unigram_probs(self)\n",
    "\n",
    "This method simply calculates the probability of each token by dividing that token by the total token count.\n",
    "\n",
    "### _generate_bigram_probs(self)\n",
    "\n",
    "This method calculates the probability of a bigram by dividing the count of that bigram over the count of the 1st word in the bigram\n",
    "\n",
    "### _generate_trigram_probs(self)\n",
    "\n",
    "This method calculates the probability of a trigram by dividing the count of that trigram over the count of the 1st 2 words in the trigram\n",
    "\n",
    "### _get_bigram_probability(self, bigram) and _get_trigram_probability(self, trigram)\n",
    "\n",
    "These methods simply access the probability dictionaries and return the value of the key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_model_abc import LanguageModel\n",
    "\n",
    "class VanillaLM(LanguageModel):\n",
    "    def _default_uni_value(self):\n",
    "        return 0.0\n",
    "\n",
    "    def _generate_unigram_probs(self):\n",
    "        total_tokens = float(sum(self.uni_count.values()))\n",
    "        for key in self.uni_count:\n",
    "            self.uni_probabilities[key] = self.uni_count[key] / total_tokens\n",
    "\n",
    "    def _generate_bigram_probs(self):\n",
    "        for key in self.bi_count:\n",
    "            words = tuple(key.split())\n",
    "            self.bi_probabilities[words] = self.bi_count[key] / self.uni_count[words[0]]\n",
    "\n",
    "    def _generate_trigram_probs(self):\n",
    "        for key in self.tri_count:\n",
    "            words = tuple(key.split())\n",
    "            bi_gram_key = words[0] + \" \" + words[1]\n",
    "            self.tri_probabilities[words] = self.tri_count[key] / self.bi_count[bi_gram_key]\n",
    "\n",
    "    def _get_bigram_probability(self, bigram):\n",
    "        return self.bi_probabilities[bigram]\n",
    "\n",
    "    def _get_trigram_probability(self, trigram):\n",
    "        return self.tri_probabilities[trigram]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace LM\n",
    "\n",
    "This class extends the LM ABC but has slightly more complicated implementations of the abstract methods. It also has a small adjustment to the methods that calculate the probability of a sentence. In my project a sentence I have treated a sentence as the contents of a s tag in the xml files. Thus, some sentences are extremely long. This combined with the fact that now we are accounting for unkown words by returning an extremely low probability for them, means that sometimes the probability of a sentence will be too small to be represented in python. When this happens I am simply returning the minimum float that python can represent instead.\n",
    "I opted for this method because if only 1 a sentence has 0 probability it will cause the perplexity of that model to be infinity.\n",
    "\n",
    "### _default_uni_value(self)\n",
    "\n",
    "This method is used as the default value for the default dictionary o the unigram probabilities dictionary. \n",
    "It simply returns\n",
    "$$\\frac{1}{TotalTokens + V}$$ \n",
    "where V is the amount of unique tokens in the training data set.\n",
    "\n",
    "### _generate_unigram_probs(self), _generate_bigram_probs(self) and _generate_trigram_probs(self)\n",
    "\n",
    "These methods populate the probability dictionaries using the calculation explained in the lecture notes.\n",
    "\n",
    "### _get_bigram_probability(self, bigram) and _get_trigram_probability(self, trigram)\n",
    "\n",
    "These methods either return the probability of the ngram, or calculate the probability of an unseen word using the formula explained in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from language_model_abc import LanguageModel\n",
    "\n",
    "class LaplaceLM(LanguageModel):\n",
    "    def _default_uni_value(self):\n",
    "        return float(1 / sum(self.uni_count.values()) + len(self.uni_count))\n",
    "\n",
    "    def _generate_unigram_probs(self):\n",
    "        total_tokens = float(sum(self.uni_count.values()))\n",
    "        for key in self.uni_count:\n",
    "            self.uni_probabilities[key] = ((self.uni_count[key] + 1)\n",
    "                                           / (total_tokens + len(self.uni_count)))\n",
    "\n",
    "    def _generate_bigram_probs(self):\n",
    "        for key in self.bi_count:\n",
    "            words = tuple(key.split())\n",
    "            self.bi_probabilities[words] = ((self.bi_count[key] + 1)\n",
    "                                            / (self.uni_count[words[0]] + len(self.uni_count)))\n",
    "\n",
    "    def _generate_trigram_probs(self):\n",
    "        for key in self.tri_count:\n",
    "            words = tuple(key.split())\n",
    "            bi_gram_key = words[0] + \" \" + words[1]\n",
    "            self.tri_probabilities[words] = ((self.tri_count[key] + 1)\n",
    "                                             / (self.bi_count[bi_gram_key] + len(self.uni_count)))\n",
    "\n",
    "    def _get_bigram_probability(self, bigram):\n",
    "        return self.bi_probabilities.get(bigram,\n",
    "                                         1 / (self.uni_count.get(bigram[0], 1)\n",
    "                                              + len(self.uni_count)))\n",
    "\n",
    "    def _get_trigram_probability(self, trigram):\n",
    "        return self.tri_probabilities.get(trigram,\n",
    "                                          1 / (self.bi_count.get(trigram[:2], 1)\n",
    "                                               + len(self.uni_count)))\n",
    "\n",
    "    def uni_sentence_probability(self, words):\n",
    "        return max(super().uni_sentence_probability(words), sys.float_info.min)\n",
    "\n",
    "    def bi_sentence_probability(self, words):\n",
    "        return max(super().bi_sentence_probability(words), sys.float_info.min)\n",
    "\n",
    "    def tri_sentence_probability(self, words):\n",
    "        return max(super().tri_sentence_probability(words), sys.float_info.min)\n",
    "\n",
    "    def sentence_probability(self, words):\n",
    "        return max(super().sentence_probability(words), sys.float_info.min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNK LM\n",
    "\n",
    "This LM also prevents a sentence probability from being 0 and it implements _defualt_uni_value(self) in the same way as the Laplace LM. It has its own versions of _get_counts(self) and _generate_counts(self) since the token counts will be different for this model. This model also has another attribute vocabulary which is a set of all the unique words that it has seen.\n",
    "\n",
    "All of the abstract methods of this method are implemented in the same way as the Laplace LM. This is as the abstract methods are all related to probability calculations.\n",
    "\n",
    "I also implemented a new version of the handle_sentence function that accepts a dictionary of words that are unknown as a parameter. Then when generating the counts any word that is in the unkown tokens dictionary gets set to *UNK*.\n",
    "\n",
    "### _generate_counts(self)\n",
    "\n",
    "The method is slightly different to the initial implementation of the LM ABC. First it generates the uni gram counts of the traning set, then it adds any token with 2 or less occurences to an unknown tokens dictionary. Finally it generates the ngram counts as normal but replacing all of the unkown tokens with *UNK*.\n",
    "\n",
    "### text_generator(self, sentence, choice), uni_sentence_probability(self, words), bi_sentence_probability(self, words), tri_sentence_probability(self, words) and sentence_probability(self, words)\n",
    "\n",
    "These methods are the same as the ABC's implementation, however, the sentences need to preprocessed to change any words that are not in the models vocabulary to UNK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from vanilla import VanillaLM\n",
    "from dataset_functions import handle_sentence, handle_sentence_unk\n",
    "\n",
    "def handle_sentence_unk(sentence_node, number_of_words, counts, unknown_tokens):\n",
    "    text = retrieve_text(sentence_node)\n",
    "    if text.strip() != \"\":\n",
    "        text = (\"<s> \" * number_of_words) + text + (\" </s>\")\n",
    "        words = text.split()\n",
    "        for index in range(len(words) - number_of_words + 1):\n",
    "            if words[index] in unknown_tokens:\n",
    "                words[index] = \"<UNK>\"\n",
    "            if number_of_words == 1:\n",
    "                n_gram = words[index]\n",
    "            else:\n",
    "                n_gram = \" \".join(words[index:index + number_of_words])\n",
    "            counts[n_gram] += 1\n",
    "\n",
    "class UnkLM(VanillaLM):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vocabulary = set(self.uni_count)\n",
    "\n",
    "    def _defualt_uni_value(self):\n",
    "        return float(1 / sum(self.uni_count.values()) + len(self.uni_count))\n",
    "\n",
    "    def _get_counts(self):\n",
    "        if not (os.path.exists('n_grams/unk/1_gram_counts.json')\n",
    "                and os.path.exists('n_grams/unk/2_gram_counts.json')\n",
    "                and os.path.exists('n_grams/unk/3_gram_counts.json')):\n",
    "            self._generate_counts()\n",
    "\n",
    "        with open(\"n_grams/unk/1_gram_counts.json\", 'r', encoding='utf-8') as fp:\n",
    "            self.uni_count = json.load(fp)\n",
    "        with open(\"n_grams/unk/2_gram_counts.json\", 'r', encoding='utf-8') as fp:\n",
    "            self.bi_count = json.load(fp)\n",
    "        with open(\"n_grams/unk/3_gram_counts.json\", 'r', encoding='utf-8') as fp:\n",
    "            self.tri_count = json.load(fp)\n",
    "\n",
    "    def _generate_counts(self):\n",
    "        n_gram_counts = defaultdict(int)\n",
    "        tree = ET.parse('../data/training_set.xml')\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            handle_sentence(child, 1, n_gram_counts)\n",
    "\n",
    "        unknown_tokens = {key for key, count in n_gram_counts.items() if count <= 2}\n",
    "\n",
    "        # Generate real counts:\n",
    "        for number_of_words in range(1, 4):\n",
    "            n_gram_counts = defaultdict(int)\n",
    "            for child in root:\n",
    "                handle_sentence_unk(child, number_of_words, n_gram_counts, unknown_tokens)\n",
    "\n",
    "            with open(f'n_grams/unk/{number_of_words}_gram_counts.json',\n",
    "                    'w', encoding='utf-8') as fp:\n",
    "                json.dump(n_gram_counts, fp, indent=4)\n",
    "\n",
    "        print(self.uni_count[\"<UNK>\"])\n",
    "\n",
    "    def _generate_unigram_probs(self):\n",
    "        total_tokens = float(sum(self.uni_count.values()))\n",
    "        for key in self.uni_count:\n",
    "            self.uni_probabilities[key] = ((self.uni_count[key] + 1)\n",
    "                                           / (total_tokens + len(self.uni_count)))\n",
    "\n",
    "    def _generate_bigram_probs(self):\n",
    "        for key in self.bi_count:\n",
    "            words = tuple(key.split())\n",
    "            self.bi_probabilities[words] = ((self.bi_count[key] + 1)\n",
    "                                            / (self.uni_count[words[0]] + len(self.uni_count)))\n",
    "\n",
    "    def _generate_trigram_probs(self):\n",
    "        for key in self.tri_count:\n",
    "            words = tuple(key.split())\n",
    "            bi_gram_key = words[0] + \" \" + words[1]\n",
    "            self.tri_probabilities[words] = ((self.tri_count[key] + 1)\n",
    "                                             / (self.bi_count[bi_gram_key] + len(self.uni_count)))\n",
    "\n",
    "    def _get_bigram_probability(self, bigram):\n",
    "        return self.bi_probabilities.get(bigram,\n",
    "                                         1 / (self.uni_count.get(bigram[0], 1)\n",
    "                                              + len(self.uni_count)))\n",
    "\n",
    "    def _get_trigram_probability(self, trigram):\n",
    "        return self.tri_probabilities.get(trigram,\n",
    "                                          1 / (self.bi_count.get(trigram[:2], 1)\n",
    "                                               + len(self.uni_count)))\n",
    "\n",
    "    def text_generator(self, sentence, choice):\n",
    "        sentence = self._remove_punctuation(sentence)\n",
    "        sentence = sentence.lower().split()\n",
    "        for index, word in enumerate(sentence):\n",
    "            if word not in self.vocabulary:\n",
    "                sentence[index] = \"<UNK>\"\n",
    "        return super().text_generator(sentence, choice)\n",
    "\n",
    "    def uni_sentence_probability(self, words):\n",
    "        words = self._remove_punctuation(words.lower())\n",
    "        words = words.split()\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in self.vocabulary:\n",
    "                words[index] = \"<UNK>\"\n",
    "        return max(super().uni_sentence_probability(words), sys.float_info.min)\n",
    "\n",
    "    def bi_sentence_probability(self, words):\n",
    "        words = self._remove_punctuation(words.lower())\n",
    "        words = [\"<s>\"] + words.split() + [\"</s>\"]\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in self.vocabulary:\n",
    "                words[index] = \"<UNK>\"\n",
    "        return max(super().bi_sentence_probability(words), sys.float_info.min)\n",
    "\n",
    "    def tri_sentence_probability(self, words):\n",
    "        words = self._remove_punctuation(words.lower())\n",
    "        words = [\"<s>\", \"<s>\"] + words.split() + [\"</s>\"]\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in self.vocabulary:\n",
    "                words[index] = \"<UNK>\"\n",
    "        return max(super().tri_sentence_probability(words), sys.float_info.min)\n",
    "\n",
    "    def sentence_probability(self, words):\n",
    "        words = self._remove_punctuation(words.lower())\n",
    "        words = [\"<s>\", \"<s>\"] + words.split() + [\"</s>\"]\n",
    "        for index, word in enumerate(words):\n",
    "            if word not in self.vocabulary:\n",
    "                words[index] = \"<UNK>\"\n",
    "        return max(super().sentence_probability(words), sys.float_info.min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM Training Time and Space Requirements\n",
    "\n",
    "To test how much time was needed to train the models 2 tests were done, the first was when the word counts were not already created and then with them created. \n",
    "The results of training the models and their space requirements are below:\n",
    "\n",
    "### Time\n",
    "No counts ready:\n",
    "Vanilla: 27.697623014450073 seconds\n",
    "Laplace: 27.66264796257019 seconds\n",
    "UNK: 18.40277075767517 seconds\n",
    "\n",
    "Counts ready:\n",
    "Vanilla: 3.051021099090576 Seconds\n",
    "Laplace: 3.132711887359619 Seconds\n",
    "Unk: 2.9299330711364746 Seconds\n",
    "\n",
    "### Space\n",
    "To calculate the space each model used I created the method below for the LM ABC.\n",
    "\n",
    "Vanilla: 744142138 bytes\n",
    "Laplace: 744142138 bytes\n",
    "UNK: 729590527 bytes\n",
    "\n",
    "### Findings\n",
    "From my findings it is evident that the trigram model for vanilla and unknown models is very expensive to calculate. Through further investigation I found that this is due to all the single and double trigram occurences that occur that the UNK model doesnt have to deal with. Also I was initially quite surprised with the space demands, but it makes sense when considering the size of the training data and the fact that I am storing it 6 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def calculate_space_needed(self):\n",
    "    size = sys.getsizeof(self.uni_count)        \n",
    "    for key, value in self.uni_count.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "\n",
    "    size += sys.getsizeof(self.bi_count)\n",
    "    for key, value in self.bi_count.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "    \n",
    "    size += sys.getsizeof(self.tri_count)\n",
    "    for key, value in self.tri_count.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "    \n",
    "    size += sys.getsizeof(self.uni_probabilities)\n",
    "    for key, value in self.uni_probabilities.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "\n",
    "    size += sys.getsizeof(self.bi_probabilities)\n",
    "    for key, value in self.bi_probabilities.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "    \n",
    "    size += sys.getsizeof(self.tri_probabilities)\n",
    "    for key, value in self.tri_probabilities.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "        \n",
    "    print(size)\n",
    "\n",
    "def calculate_space_needed_unk(self):\n",
    "    size = sys.getsizeof(self.uni_count)        \n",
    "    for key, value in self.uni_count.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "\n",
    "    size += sys.getsizeof(self.bi_count)\n",
    "    for key, value in self.bi_count.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "    \n",
    "    size += sys.getsizeof(self.tri_count)\n",
    "    for key, value in self.tri_count.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "    \n",
    "    size += sys.getsizeof(self.uni_probabilities)\n",
    "    for key, value in self.uni_probabilities.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "\n",
    "    size += sys.getsizeof(self.bi_probabilities)\n",
    "    for key, value in self.bi_probabilities.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "    \n",
    "    size += sys.getsizeof(self.tri_probabilities)\n",
    "    for key, value in self.tri_probabilities.items():\n",
    "        size += sys.getsizeof(key)\n",
    "        size += sys.getsizeof(value)\n",
    "    \n",
    "    size += sys.getsizeof(self.vocabulary)\n",
    "    for key in self.vocabulary:\n",
    "        size += sys.getsizeof(key)\n",
    "        \n",
    "    print(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexities\n",
    "I calculated perplexity of the data set by averaging the perplexities of each sentence. This was calculated using the following formula:\n",
    "\n",
    "$$PP(S) = {\\frac{1}{P(S)}}^{\\frac{1}{N}}$$\n",
    "where N is the number of words in the sentence\n",
    "\n",
    "| | **Unigram** | **Bigram** | **Trigram** | **Linear Interpolation** |\n",
    "| ------- | ------- | -------- | -------- | --------- |\n",
    "| **Vanilla** | INFINITY | INFINITY | INFINITY | INFINITY |\n",
    "| **Laplace** | 8354.61 | 9615.34 | 22976.01 | 5268.14 |\n",
    "| **UNK** | 5603.24 | 5965.04 | 10951.60 | 3515.70|\n",
    "\n",
    "The infinity values obtained are due to the vanilla lm encountering unknown words. When this happens the the perplexity approaches infinity due the division by zero.\n",
    "The other models perplexity values are a bit skewed due to the fact that for some sentences I am not returning their true probability, as this is smaller than python can represent using a float.\n",
    "\n",
    "## Sentence Probabilities\n",
    "\n",
    "For this section the first test sentence I used is this:\n",
    "\n",
    "\"ocean environmental management has applied for a 30,000-tonne plant at seal sands teesside and is appealing against the local authority's refusal to allow it\"\n",
    "\n",
    "bosnian refugees assured of asylum\n",
    "\n",
    "Here is a table with the sentence probabilities:\n",
    "\n",
    "| | **Unigram** | **Bigram** | **Trigram** | **Linear Interpolation** |\n",
    "| ------- | ------- | -------- | -------- | --------- |\n",
    "| **Vanilla** | 0.0 | 0.0 | 0.0 | 0.0 |\n",
    "| **Laplace** | 4.9653571e-77 | 6.8572746e-99 | 8.8640346e-116 | 5.2145152e-86 |\n",
    "| **UNK** | 1.31012857e-83 | 6.44653e-91 | 3.321860e-107 | 4.20396477e-88|\n",
    "\n",
    "I also tried it with this sentence to try and gain some values from the Vanilla lm.\n",
    "\n",
    "\"bosnian refugees assured of asylum\"\n",
    "\n",
    "| | **Unigram** | **Bigram** | **Trigram** | **Linear Interpolation** |\n",
    "| ------- | ------- | -------- | -------- | --------- |\n",
    "| **Vanilla** | 4.3879832e-23 | 0.0 | 0.0 | 6.770046e-26 |\n",
    "| **Laplace** | 4.68944600e-23 | 5.1685384e-25 | 3.3177554e-25 | 4.0163971e-23 |\n",
    "| **UNK** | 4.99982e-23 | 1.09758518e-23 | 1.16178394e-23 | 4.9647802e-22|\n",
    "\n",
    "The results are consistent, The more specific the model gets in terms of context the lower the probability of a sentence occuring. This is due simply to the massive increase in the amount of combinations of words that can occur. \n",
    "\n",
    "## Text generation\n",
    "\n",
    "The phrase I selected initially to prompt the models with is the first 3 words of a sentence in my test set:\n",
    "\n",
    "\"what is the\"\n",
    "\n",
    "### Vanilla\n",
    "\n",
    "- Uni: what is the this monogram manager hence that be the telephoned can't presentable jones more bridgeborough skin had up eleven from to joint-owner to he known breaking\n",
    "- Bi: what is the agency ownership can be left\n",
    "- Tri: what is the subject\n",
    "- Linear Interpolation: what is the most private of all those years ago\n",
    "\n",
    "### Laplace\n",
    "\n",
    "- Uni: what is the doreen created forms obesity familiar spaces east they times else essential worry\n",
    "- Bi: what is the user is sold out four bedrooms\n",
    "- Tri: what is the boss of coach giant national express closed at a big house\n",
    "- Linear Interpolation: what is the value the right\n",
    "\n",
    "### UNK\n",
    "\n",
    "- Uni: what is the these mine current an management what the give managed\n",
    "- Bi: what is the equation 18 sartre how you can give consumers have been denied the middle aged golden girl at 7.45pm\n",
    "- Tri: what is the probable outcome\n",
    "- Linear Interpolation: what is the\n",
    "\n",
    "### Observations\n",
    "\n",
    "Here are a few observations of my results:\n",
    "\n",
    "1. Unigrams as expected make no sense.\n",
    "2. Unigrams are also the longest sentences as expected.\n",
    "3. As the ngram size increases, the sequences of words produced seem to make more sense, it gets noticeably better at each step.\n",
    "4. Obviously since the predicted word is random, the above obvservations are not true 100% of the time.\n",
    "\n",
    "\n",
    "## Final Project\n",
    "When running the main file of my project, after the models have trained the user is prompted whether they would like to generate text, or calculate the probability of a sentence. Next they are prompted to choose their model. In the case that they choose text generation, they are also prompted to decide whether they would like to generate based on unigram, bigram, trigram or linear interpolation.\n",
    "\n",
    "I opted for this approach since I found it more interesting to be able to compare each level of each model.\n",
    "\n",
    "## Testing \n",
    "\n",
    "Throughout the development of my code I tested each section rigorously. Any large issue that I encountered such as sentence probabilities being too small has already been documented and explianed.\n",
    "\n",
    "## References\n",
    "1. Lecture notes - https://www.um.edu.mt/vle/pluginfile.php/1356879/mod_resource/content/1/Lecture%20Slides%20-%20Language%20Modelling.pdf\n",
    "2. Inspiration for text generator section - https://github.com/aduroy/n-gram-generator/tree/master\n",
    "3. sentence probability research - https://cs.stackexchange.com/questions/47502/computing-probability-of-sentence-using-n-grams#:~:text=The%20N%2Dgram%20model%20assumes,N%2B1%7Cw2%E2%80%A6\n",
    "4. perplexity research - https://towardsdatascience.com/perplexity-of-language-models-revisited-6b9b4cf46792"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
