{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation explenation\n",
    "\n",
    "Below is my implementation for producing the n_grams' frequency counts. It consists of 4 different functions:\n",
    "- frequency_counts()\n",
    "- traverse_tree(node, number_of_words, counts)\n",
    "- handle_sentence(sentence_node, number_of_words, counts)\n",
    "- retrieve_text(node)\n",
    "\n",
    "### frequency_counts()\n",
    "This function starts by parsing either a single or multiple *.xml* files which contain the corpus used as training data for this project. Once the root of each *.xml* file has been found, the function iterates over a loop three times, generating a frequency count for each n_gram *(unigram, bigram, trigram)*. \n",
    "This is achieved by calling the traverse_tree() function for each of the roots children. \n",
    "\n",
    "\n",
    "Once the frequency count dictionary has been created, it is stored in a json file so that in the future this process doesn't need to be repeated everytime the language model is to be used.\n",
    "\n",
    "### traverse_tree(node, number_of_words, counts)\n",
    "This function takes 3 parameters:\n",
    "- node (xml.etree.ElementTree.Element): The current node in the XML tree.\n",
    "- number_of_words (int): The number of words in the n-grams to be counted.\n",
    "- counts (collections.defaultdict): A dictionary to store the n-gram counts.\n",
    "\n",
    "\n",
    "This function is a recursive depth first function that traverses the tree until it finds either a *teiHeader* tag or an *s* tag. Due to the structure of the *.xml* files, the first child of the root (*teiHeader*) contains information about the text and its origins. This is irrelevant to our frequency counts so we simply ignore it and keep searching the tree until an *s* tag is found. Once found the handle_sentence() function is called.\n",
    "\n",
    "### handle_sentence(sentence_node, number_of_words, counts)\n",
    "This function takes 3 parameters:\n",
    "- sentence_node (xml.etree.ElementTree.Element): The current sentence node in the XML tree.\n",
    "- number_of_words (int): The number of words in the n-grams to be counted.\n",
    "- counts (collections.defaultdict): A dictionary to store the n-gram counts.\n",
    "  \n",
    "The function adds an end *s* tag to the string returned by retrieve_text(), processes each sentence and computes and updates n-gram frequencies stored in counts. It will only update n-gram frequencies if the sentence contained words, if no words were present then it moves onto the next sentence.\n",
    "\n",
    "### retrieve_text(node)\n",
    "This function takes one parameter:\n",
    "- node (xml.etree.ElementTree.Element): The current node in the XML tree.\n",
    "\n",
    "It returns a str, which represents the concatenated text from the node and its descendants.\n",
    "\n",
    "This function extracts and concatenates text from XML nodes, adding start markers to each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "def frequency_counts():\n",
    "    training_dir_path = '../corpus/train/'\n",
    "    xml_files = [f for f in os.listdir(training_dir_path) if f.endswith('.xml')]\n",
    "\n",
    "    # Create frequency counts for unigrams, bigrams, and trigrams and save to a json file\n",
    "    for number_of_words in range(1, 4):\n",
    "        n_gram_counts = defaultdict(int)\n",
    "\n",
    "        for file in xml_files:\n",
    "            path = os.path.join(training_dir_path, file)\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                traverse_tree(child, number_of_words, n_gram_counts)\n",
    "\n",
    "        with open(f'../src/n_grams/{number_of_words}_gram_counts.json', 'w', encoding='utf-8') as fp:\n",
    "            json.dump(n_gram_counts, fp)\n",
    "\n",
    "\n",
    "def traverse_tree(node, number_of_words, counts):\n",
    "    if node.tag != 'teiHeader':\n",
    "        for child in node:\n",
    "            if child.tag == 's':\n",
    "                handle_sentence(child, number_of_words, counts)\n",
    "            else:\n",
    "                traverse_tree(child, number_of_words, counts)\n",
    "\n",
    "\n",
    "def handle_sentence(sentence_node, number_of_words, counts):\n",
    "    text = retrieve_text(sentence_node) + \" </s>\"\n",
    "    if text != \"<s> \":\n",
    "        words = text.split()\n",
    "        for index in range(len(words) - number_of_words + 1):\n",
    "            if number_of_words == 1:\n",
    "                n_gram = words[index]\n",
    "            else:\n",
    "                n_gram = \" \".join(words[index:index + number_of_words])\n",
    "            counts[n_gram] += 1\n",
    "\n",
    "\n",
    "def retrieve_text(node):\n",
    "    text = \"<s> \"\n",
    "    for child in node:\n",
    "        if child.tag == 'w':\n",
    "            if child.text:\n",
    "                text += child.text.lower()\n",
    "        else:\n",
    "            if len(node) > 0:\n",
    "                for grandchild in child:\n",
    "                    text += retrieve_text(grandchild)\n",
    "    return text\n",
    "\n",
    "\n",
    "frequency_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation stages\n",
    "\n",
    "The above is the final edition of the code. Initially it was developed to only handle one xml file from the test corpus, this was later extended to handle the whole training set, contained in the *corpus/train/* directory. \n",
    "\n",
    "Below I have implemented said *single file* implementation, along with a modified version of the final product that does not change or generate the json files. These 2 functions will be used later in our testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def single_file_frequency_counts(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Create frequency counts for unigrams, bigrams, and trigrams\n",
    "    for number_of_words in range(1, 4):\n",
    "        n_gram_counts = defaultdict(int)\n",
    "        for child in root:\n",
    "            traverse_tree(child, number_of_words, n_gram_counts)\n",
    "            \n",
    "def corpus_frequency_counts():\n",
    "    test_dir_path = \"test_3/\"\n",
    "    xml_files = [f for f in os.listdir(test_dir_path) if f.endswith('.xml')]\n",
    "\n",
    "        # Create frequency counts for unigrams, bigrams, and trigrams\n",
    "    for number_of_words in range(1, 4):\n",
    "        n_gram_counts = defaultdict(int)\n",
    "\n",
    "        for xml_file in xml_files:\n",
    "            path = os.path.join(test_dir_path, xml_file)\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                traverse_tree(child, number_of_words, n_gram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Before extending the initial *single file* implementation, a short 3 line extract of one of the training files was used and the json files were checked to verify that the generated counts were correct.\n",
    "\n",
    "\n",
    "Finally once the implemenation was finished, the performance of this section was tested using 2 different metrics.\n",
    "1. Time taken to create each n_gram\n",
    "2. CPU and RAM usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1:\n",
      "Time to process 1_gram: 1.6927719116210938e-05 seconds\n",
      "Time to process 2_gram: 7.867813110351562e-06 seconds\n",
      "Time to process 3_gram: 4.76837158203125e-06 seconds\n",
      "\n",
      "Test 2:\n",
      "Time to process 1_gram: 0.010452032089233398 seconds\n",
      "Time to process 2_gram: 0.010739803314208984 seconds\n",
      "Time to process 3_gram: 0.010126829147338867 seconds\n",
      "\n",
      "Test 3:\n",
      "Time to process 1_gram: 5.182066917419434 seconds\n",
      "Time to process 2_gram: 6.037155866622925 seconds\n",
      "Time to process 3_gram: 6.28548789024353 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def timed_single_file_frequency_counts(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    text_node = root.find('.//wtext')\n",
    "\n",
    "    # Create frequency counts for unigrams, bigrams, and trigrams and save to a json file\n",
    "    for number_of_words in range(1, 4):\n",
    "        start_time = time.time()\n",
    "\n",
    "        n_gram_counts = defaultdict(int)\n",
    "        traverse_tree(text_node, number_of_words, n_gram_counts)\n",
    "\n",
    "        end_time = time.time()  \n",
    "        print(f\"Time to process {number_of_words}_gram: {end_time - start_time} seconds\")\n",
    "\n",
    "def timed_corpus_frequency_counts():\n",
    "    test_dir_path = \"test_3/\"\n",
    "    xml_files = [f for f in os.listdir(test_dir_path) if f.endswith('.xml')]\n",
    "\n",
    "    # Create frequency counts for unigrams, bigrams, and trigrams\n",
    "    for number_of_words in range(1, 4):\n",
    "        start_time = time.time()\n",
    "\n",
    "        n_gram_counts = defaultdict(int)\n",
    "\n",
    "        for file in xml_files:\n",
    "            path = os.path.join(test_dir_path, file)\n",
    "            tree = ET.parse(path)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                traverse_tree(child, number_of_words, n_gram_counts)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Time to process {number_of_words}_gram: {end_time - start_time} seconds\")\n",
    "\n",
    "print(\"Test 1:\")\n",
    "timed_single_file_frequency_counts(\"test_1.xml\")\n",
    "print()\n",
    "\n",
    "print(\"Test 2:\")\n",
    "timed_single_file_frequency_counts(\"test_2.xml\")\n",
    "print()\n",
    "\n",
    "print(\"Test 3:\")\n",
    "timed_corpus_frequency_counts()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few lines test:\n",
      "CPU Usage: 154.3%\n",
      "RAM Usage: 0.0 MB\n",
      "Single file test:\n",
      "CPU Usage: 99.2%\n",
      "RAM Usage: 0.921875 MB\n",
      "Corpus Test:\n",
      "CPU Usage: 99.6%\n",
      "RAM Usage: 56.125 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def test_cpu_usage_single_file(path):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_cpu = process.cpu_percent()\n",
    "\n",
    "    single_file_frequency_counts(path)\n",
    "\n",
    "    end_cpu = process.cpu_percent()\n",
    "\n",
    "    cpu_usage = end_cpu - start_cpu\n",
    "\n",
    "    print(f\"CPU Usage: {cpu_usage}%\")\n",
    "\n",
    "def test_ram_usage_single_file(path):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    single_file_frequency_counts(path)\n",
    "\n",
    "    end_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    ram_usage = end_ram - start_ram\n",
    "\n",
    "    print(f\"RAM Usage: {ram_usage} MB\")\n",
    "\n",
    "def test_cpu_usage_corpus():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_cpu = process.cpu_percent()\n",
    "\n",
    "    corpus_frequency_counts()\n",
    "\n",
    "    end_cpu = process.cpu_percent()\n",
    "\n",
    "    cpu_usage = end_cpu - start_cpu\n",
    "\n",
    "    print(f\"CPU Usage: {cpu_usage}%\")\n",
    "\n",
    "def test_ram_usage_corpus():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    corpus_frequency_counts()\n",
    "\n",
    "    end_ram = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "    ram_usage = end_ram - start_ram\n",
    "\n",
    "    print(f\"RAM Usage: {ram_usage} MB\")\n",
    "\n",
    "print(\"Few lines test:\")\n",
    "test_cpu_usage_single_file(\"test_1.xml\")\n",
    "test_ram_usage_single_file(\"test_1.xml\")\n",
    "\n",
    "print(\"Single file test:\")\n",
    "test_cpu_usage_single_file(\"test_2.xml\")\n",
    "test_ram_usage_single_file(\"test_2.xml\")\n",
    "\n",
    "print(\"Corpus Test:\")\n",
    "test_cpu_usage_corpus()\n",
    "test_ram_usage_corpus()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
